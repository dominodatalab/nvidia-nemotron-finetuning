{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8afb7a8-9e95-48c5-9b60-0f70b41844b0",
   "metadata": {},
   "source": [
    "# Fine-Tuning Nemotron-3 8B using Low-Rank Adaptation (LoRA)\n",
    "Nemotron-3 is a robust, powerful family of Large Language Models that can provide compelling responses on a wide range of tasks. While the 8B parameter base model serves as a strong baseline for multiple downstream tasks, they can lack in domain-specific knowledge or proprietary or otherwise sensitive information. Fine-tuning is often used as a means to update a model for a specific task or tasks to better respond to domain-specific prompts. This notebook walks through preparing a dataset and using Low Rank Adaptation (LoRA) to fine-tune the base Nemotron-3 8B model from Hugging Face against the dataset.\n",
    "\n",
    "The implementation of LoRA is based on the paper, [LoRA: Low-Rank Adaptation of Large Language Models](https://openreview.net/pdf?id=nZeVKeeFYf9) by Hu et al."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e390b1b7-d923-4871-8d95-8651f1940b0f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Getting the model\n",
    "You will need to request access to the [Nemotron-3-8B-Base-4K Model](https://huggingface.co/nvidia/nemotron-3-8b-base-4k) through Hugging Face. \n",
    "\n",
    "Once you have access, set the your Hugging Face username and access token accordingly and run the below cell to download the model into the artifact store. \n",
    "\n",
    "Optionally, you can also download this into an external data volume for better portability & longer term storage. If you choose to do so, make sure to change the `MODEL_PATH` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c58b817-aa8d-4563-ae90-317680b1ddef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "HF_USERNAME = \"<HUGGING-FACE-USERNAME>\"\n",
    "HF_ACCESS_TOKEN = \"<HUGGING-ACCESS-TOKEN>\" # For best practice, set this as an environment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cc5df2-01e0-4f2b-8239-c9864c8bfd5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import subprocess\n",
    "\n",
    "MODEL_PATH = \"/mnt/artifacts/nemotron/Nemotron-3-8B-Base-4k.nemo\"\n",
    "\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    subprocess.run(\"curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash\", shell=True, check=True)\n",
    "    subprocess.run(\"sudo apt-get install git-lfs\", shell=True, check=True)\n",
    "    subprocess.run(f\"git clone https://{HF_USERNAME}:{HF_ACCESS_TOKEN}@huggingface.co/nvidia/nemotron-3-8b-base-4k /mnt/artifacts/nemotron\", shell=True, check=True)\n",
    "else:\n",
    "    print(f\"The Nemotron model already exists. Skipping download ... \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2f81f1-653e-482c-b3c5-92d5cbde6b68",
   "metadata": {},
   "source": [
    "# Preparing The Dataset\n",
    "We will be using LoRA to teach our model to do Extractive Question Answering. The dataset being used for fine-tuning needs to be converted to a .jsonl file and follow a specific format. In general, question and answer datasets are easiest to work with by providing context (if applicable), a question, and the expected answer, though different downstream tasks work as well.\n",
    "\n",
    "### Downloading the dataset\n",
    "We will be using the [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/) reading comprehension dataset, consisting of questions posed by crowd workers on a set of Wikipedia articles, where the answer to every question is a segment of text. More information on [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/) can be found on their website or in their paper by Rajpurkar et. al \"[Know What You Donâ€™t Know: Unanswerable Questions for SQuAD](https://arxiv.org/pdf/1806.03822.pdf)\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2227986d-b843-4ecb-bfd5-3e1b7dc905d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"/mnt/code/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae986e6-f0a2-4ddf-ba1c-6916e0597f62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import wget\n",
    "import sys\n",
    "\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '8'\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "SQUAD_DIR = os.path.join(DATA_DIR, \"SQuAD\")\n",
    "os.makedirs(SQUAD_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6b21ba-1743-4728-9150-d977dc614b54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download the SQuAD dataset\n",
    "!wget -nc https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\n",
    "!wget -nc https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\n",
    "!mv train-v1.1.json {SQUAD_DIR}\n",
    "!mv dev-v1.1.json {SQUAD_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e273888-edde-4041-9721-6a1ce7f885d0",
   "metadata": {},
   "source": [
    "### Preprocessing the dataset\n",
    "Datasets often need some form of preprocessing to convert it into a form ready for fine-tuning. LoRA (and all PEFT tuning) models expect at least two fields in the jsonl files. The `input` field should contain all the tokens necessary for the model to generate the `output`. For example for extractive QA, the `input` should contain the context text as well as the question.\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"input\": \"User: Context: [CONTEXT_1] Question: [QUESTION_1]\\n\\nAssistant:\", \"output\": [ANSWER_1]},\n",
    "    {\"input\": \"User: Context: [CONTEXT_2] Question: [QUESTION_2]\\n\\nAssistant:\", \"output\": [ANSWER_2]},\n",
    "    {\"input\": \"User: Context: [CONTEXT_3] Question: [QUESTION_3]\\n\\nAssistant:\", \"output\": [ANSWER_3]},\n",
    "]\n",
    "```\n",
    "Note that we use keywords in the input like `Context:`, `Question:` to separate the text representing the context and question. We also use the keyword `User:` and end each of the input with `\\n\\nAssistant:` tokens. These are recommended because NeMo's instruction-tuned models are trained with a prefix of `User:` and suffix `\\n\\nAssistant:`.\n",
    "\n",
    "The SQuAD dataset does not already reflect this, so let's go ahead and preprocess it to fit the above format. \n",
    "\n",
    "To do so, a processing script has been included with this project template. Feel free to take a look inside the `prompt_learning_squad_preprocessing.py` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06ea9b1-7546-4125-8e5e-48bc8d491c51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preprocess squad data\n",
    "!python /opt/NeMo/scripts/dataset_processing/nlp/squad/prompt_learning_squad_preprocessing.py --sft-format --data-dir {SQUAD_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cbbe87-b081-4e2d-bb4a-58ca1536a3d4",
   "metadata": {},
   "source": [
    "Let's split the datasets into train and validation files, and take a look at a few samples of the data to confirm the preprocessing is satisfactory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9558dca-2ee2-43ec-a64f-9708d5cd6889",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# What the squad dataset looks like after processing\n",
    "! head -5000 $SQUAD_DIR/squad_train.jsonl > $SQUAD_DIR/squad_short_train.jsonl\n",
    "! head -500 $SQUAD_DIR/squad_val.jsonl > $SQUAD_DIR/squad_short_val.jsonl\n",
    "! head -4 $SQUAD_DIR/squad_short_val.jsonl\n",
    "! head -4 $SQUAD_DIR/squad_short_train.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5327710d-3835-41ec-a1a4-49fccc5dd271",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Now that the model is available and the data is prepared, we are ready to start the training.\n",
    "\n",
    "### Load Config\n",
    "\n",
    "The NeMo toolkit leverages a configuration file to make it easy to define and explore with training parameters without having to change the code. For this project template, a default configuration for fine-tuning has been included at `conf/nemotron-finetune-config.yaml` which is based off the sample configs provided by NVIDIA here: https://github.com/NVIDIA/NeMo/tree/main/examples/nlp/language_modeling/tuning/conf\n",
    "\n",
    "We will start by loading in that configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b37b20-3625-4543-94fd-3530537c1314",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "\n",
    "cfg = OmegaConf.load(\"/mnt/code/conf/nemotron-finetune-config.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecc6db8-8e5a-4b5e-92c6-67902d968a3d",
   "metadata": {},
   "source": [
    "With the config loaded, we can override certain settings for our environment. The default values should work but here are some parameter that you may want to adjust:\n",
    "\n",
    "* `config.trainer.precision` - This is the precision that will be used during fine-tuning. The model might be more accurate with higher values but it also uses more memory than lower precisions. If the fine-tuning process runs out of memory, try reducing the precision here.\n",
    "* `config.trainer.devices` - This is the number of devices that will be used. If running on a multi-GPU system, increase this number as appropriate.\n",
    "* `config.model.global_batch_size` - If using a higher GPU count or if additional GPU memory allows, this value can be increased for higher performance. Note that higher batch sizes use more GPU memory.\n",
    "\n",
    "One config that you will want to update is the `config.model.restore_from_path`. This should point to the `.nemo` file where your model is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f0a870-40e2-421a-a88d-f78f730d357c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cfg.model.restore_from_path=MODEL_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510647e8-c07f-432f-a3ce-766c37e74edb",
   "metadata": {},
   "source": [
    "By default, this notebook doesn't use distributed training so we will set some environment variables accordingly. If you do choose to use distributed training methods, you may want to change the environment variables below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d3f1a6-256b-4b87-855b-0fef507bec05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"LOCAL_RANK\"] = '0'\n",
    "os.environ[\"RANK\"] = '0'\n",
    "os.environ[\"WORLD_SIZE\"] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f614b4d-c40d-4c80-a986-fffafd96a5e7",
   "metadata": {},
   "source": [
    "### Configure Training\n",
    "\n",
    "We now load in our model and configure the trainer using the loaded config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d48817d-fb61-471a-b6e7-7591433288a6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nemo.collections.nlp.parts.megatron_trainer_builder import MegatronLMPPTrainerBuilder\n",
    "from nemo.collections.nlp.models.language_modeling.megatron_gpt_sft_model import MegatronGPTSFTModel\n",
    "from nemo.collections.nlp.parts.peft_config import LoraPEFTConfig\n",
    "\n",
    "trainer = MegatronLMPPTrainerBuilder(cfg).create_trainer()\n",
    "model_cfg = MegatronGPTSFTModel.merge_cfg_with(cfg.model.restore_from_path, cfg)\n",
    "model = MegatronGPTSFTModel.restore_from(cfg.model.restore_from_path, model_cfg, trainer=trainer)\n",
    "model.add_adapter(LoraPEFTConfig(model_cfg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ab9e65-04a7-4bc9-9460-af092ff1d482",
   "metadata": {},
   "source": [
    "### Configure experiment\n",
    "We will also activate the experiment logging so that we can create checkpoints to resume from later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db58b2b4-434f-4354-a985-91a1b3926816",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nemo.utils.exp_manager import exp_manager\n",
    "\n",
    "exp_dir = exp_manager(trainer, cfg.get(\"exp_manager\", None))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1754849-1b32-49ce-a019-7731e547673f",
   "metadata": {},
   "source": [
    "### Train model\n",
    "Lastly, we can finally train our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e9c552-fa12-460b-9db7-a41722397922",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2598875-49ca-4ab8-bdcd-bcca493ab6b7",
   "metadata": {},
   "source": [
    "# Evaluate\n",
    "Now that we have finished fine-tuning, let's try to make some predictions on it from our test dataset.\n",
    "\n",
    "### Load config\n",
    "Just like with fine-tuning, we have prepared a config for this project template. Let's start by loading that in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a71e7b-9663-48a3-ba6c-d3bac3180be2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config_eval = OmegaConf.load(\"/mnt/code/conf/nemotron-eval-config.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4f68e5-405d-42c2-b07f-b6ba15c0d707",
   "metadata": {},
   "source": [
    "We will override the model path with the last checkpoint that was logged during fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940b0214-1c59-4462-87f4-c6d2ffe28177",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH=\"/mnt/code/nemo_experiments/megatron_gpt_peft_lora_tuning/checkpoints/megatron_gpt_peft_lora_tuning.nemo\"\n",
    "config_eval.model.restore_from_path=MODEL_PATH\n",
    "config_eval.model.peft.restore_from_path=CHECKPOINT_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42742ec-bbb3-4035-b780-ce9dde94bf4e",
   "metadata": {},
   "source": [
    "### Load model\n",
    "Now we load in the model and trainer that we will use for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4380c787-5943-4345-b7f5-2068321a8924",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nemo.collections.nlp.parts.megatron_trainer_builder import MegatronTrainerBuilder\n",
    "from nemo.collections.nlp.models.language_modeling.megatron_gpt_sft_model import MegatronGPTSFTModel\n",
    "from nemo.collections.nlp.parts.peft_config import LoraPEFTConfig\n",
    "\n",
    "trainer_eval = MegatronTrainerBuilder(config_eval).create_trainer()\n",
    "eval_model_cfg = MegatronGPTSFTModel.merge_inference_cfg(config_eval.model.peft.restore_from_path, config_eval)\n",
    "model_eval = MegatronGPTSFTModel.restore_from(config_eval.model.restore_from_path, eval_model_cfg, trainer=trainer_eval)\n",
    "model_eval.load_adapters(config_eval.model.peft.restore_from_path)\n",
    "model_eval.freeze()\n",
    "\n",
    "print(\"Parameter count manually:\\n\", model_eval.summarize())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cf334c-2131-4765-a5bd-372f1fc74afa",
   "metadata": {},
   "source": [
    "### Load test dataset\n",
    "We load in the test dataset as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14eb9aff-7412-4aef-93f1-3227e05e6593",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_test_ds = model_eval._build_dataset(eval_model_cfg.data.test_ds, is_train=False)\n",
    "from torch.utils.data import DataLoader\n",
    "request_dl = DataLoader(\n",
    "    dataset=_test_ds[0],\n",
    "    batch_size=eval_model_cfg.data.test_ds.global_batch_size,\n",
    "    collate_fn=_test_ds[0].collate_fn,\n",
    ")\n",
    "config_inference = OmegaConf.to_container(config_eval.inference, resolve=True)\n",
    "model_eval.set_inference_config(config_inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f13a684-1301-49e6-b084-7bb547125b1c",
   "metadata": {},
   "source": [
    "### Run predictions\n",
    "And now it is time to run the predictions through the model and see the results!\n",
    "\n",
    "**Keep in mind the results you see may vary in quality. The hyperparameters presented in this notebook are not optimal and only serve as examples. Could you be underfitting? Overfitting? These can be adjusted in the configs to improve performance. The point is fine tuning the out-of-the-box model to the general QA task is easy and straightforward with this workflow!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0b7f22-ba26-49cc-a667-d0f46f16d4c1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = trainer_eval.predict(model_eval, request_dl)\n",
    "for batch in response:\n",
    "    for s in batch['sentences']:\n",
    "        print(f\"{s}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c651b84a-c343-40d2-9cd9-4a07b241918f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
