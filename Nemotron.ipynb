{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8afb7a8-9e95-48c5-9b60-0f70b41844b0",
   "metadata": {},
   "source": [
    "# Fine-Tuning Nemotron-3 8B using Low-Rank Adaptation (LoRA)\n",
    "Nemotron-3 is a robust, powerful family of Large Language Models that can provide compelling responses on a wide range of tasks. While the 8B parameter base model serves as a strong baseline for multiple downstream tasks, they can lack in domain-specific knowledge or proprietary or otherwise sensitive information. Fine-tuning is often used as a means to update a model for a specific task or tasks to better respond to domain-specific prompts. This notebook walks through preparing a dataset and using Low Rank Adaptation (LoRA) to fine-tune the base Nemotron-3 8B model from Hugging Face against the dataset.\n",
    "\n",
    "The implementation of LoRA is based on the paper, [LoRA: Low-Rank Adaptation of Large Language Models](https://openreview.net/pdf?id=nZeVKeeFYf9) by Hu et al."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e390b1b7-d923-4871-8d95-8651f1940b0f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Getting the model\n",
    "You will need to request access to the [Nemotron-3-8B-Base-4K Model](https://huggingface.co/nvidia/nemotron-3-8b-base-4k) through Hugging Face. \n",
    "\n",
    "Once you have access, set the your Hugging Face username and access token accordingly and run the below cell to download the model into the artifact store. \n",
    "\n",
    "Optionally, you can also download this into an external data volume for better portability & longer term storage. If you choose to do so, make sure to change the `MODEL_PATH` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c58b817-aa8d-4563-ae90-317680b1ddef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "HF_USERNAME = \"<HUGGING-FACE-USERNAME>\"\n",
    "HF_ACCESS_TOKEN = \"<HUGGING-ACCESS-TOKEN>\" # For best practice, set this as an environment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06cc5df2-01e0-4f2b-8239-c9864c8bfd5e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Nemotron model already exists. Skipping download ... \n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import subprocess\n",
    "\n",
    "MODEL_PATH = \"/mnt/artifacts/nemotron/Nemotron-3-8B-Base-4k.nemo\"\n",
    "\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    subprocess.run(\"curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash\", shell=True, check=True)\n",
    "    subprocess.run(\"sudo apt-get install git-lfs\", shell=True, check=True)\n",
    "    subprocess.run(f\"git clone https://{HF_USERNAME}:{HF_ACCESS_TOKEN}@huggingface.co/nvidia/nemotron-3-8b-base-4k /mnt/artifacts/nemotron\", shell=True, check=True)\n",
    "else:\n",
    "    print(f\"The Nemotron model already exists. Skipping download ... \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2f81f1-653e-482c-b3c5-92d5cbde6b68",
   "metadata": {},
   "source": [
    "# Preparing The Dataset\n",
    "We will be using LoRA to teach our model to do Extractive Question Answering. The dataset being used for fine-tuning needs to be converted to a .jsonl file and follow a specific format. In general, question and answer datasets are easiest to work with by providing context (if applicable), a question, and the expected answer, though different downstream tasks work as well.\n",
    "\n",
    "### Downloading the dataset\n",
    "We will be using the [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/) reading comprehension dataset, consisting of questions posed by crowd workers on a set of Wikipedia articles, where the answer to every question is a segment of text. More information on [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/) can be found on their website or in their paper by Rajpurkar et. al \"[Know What You Don’t Know: Unanswerable Questions for SQuAD](https://arxiv.org/pdf/1806.03822.pdf)\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2227986d-b843-4ecb-bfd5-3e1b7dc905d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"/mnt/code/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ae986e6-f0a2-4ddf-ba1c-6916e0597f62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import wget\n",
    "import sys\n",
    "\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '8'\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "SQUAD_DIR = os.path.join(DATA_DIR, \"SQuAD\")\n",
    "os.makedirs(SQUAD_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b6b21ba-1743-4728-9150-d977dc614b54",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-03-20 15:32:05--  https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\n",
      "Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.108.153, 185.199.111.153, 185.199.110.153, ...\n",
      "Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.108.153|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 30288272 (29M) [application/json]\n",
      "Saving to: ‘train-v1.1.json’\n",
      "\n",
      "train-v1.1.json     100%[===================>]  28.88M   111MB/s    in 0.3s    \n",
      "\n",
      "2024-03-20 15:32:05 (111 MB/s) - ‘train-v1.1.json’ saved [30288272/30288272]\n",
      "\n",
      "--2024-03-20 15:32:05--  https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\n",
      "Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.108.153, 185.199.111.153, 185.199.110.153, ...\n",
      "Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.108.153|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4854279 (4.6M) [application/json]\n",
      "Saving to: ‘dev-v1.1.json’\n",
      "\n",
      "dev-v1.1.json       100%[===================>]   4.63M  --.-KB/s    in 0.04s   \n",
      "\n",
      "2024-03-20 15:32:06 (105 MB/s) - ‘dev-v1.1.json’ saved [4854279/4854279]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download the SQuAD dataset\n",
    "!wget -nc https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\n",
    "!wget -nc https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\n",
    "!mv train-v1.1.json {SQUAD_DIR}\n",
    "!mv dev-v1.1.json {SQUAD_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e273888-edde-4041-9721-6a1ce7f885d0",
   "metadata": {},
   "source": [
    "### Preprocessing the dataset\n",
    "Datasets often need some form of preprocessing to convert it into a form ready for fine-tuning. LoRA (and all PEFT tuning) models expect at least two fields in the jsonl files. The `input` field should contain all the tokens necessary for the model to generate the `output`. For example for extractive QA, the `input` should contain the context text as well as the question.\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"input\": \"User: Context: [CONTEXT_1] Question: [QUESTION_1]\\n\\nAssistant:\", \"output\": [ANSWER_1]},\n",
    "    {\"input\": \"User: Context: [CONTEXT_2] Question: [QUESTION_2]\\n\\nAssistant:\", \"output\": [ANSWER_2]},\n",
    "    {\"input\": \"User: Context: [CONTEXT_3] Question: [QUESTION_3]\\n\\nAssistant:\", \"output\": [ANSWER_3]},\n",
    "]\n",
    "```\n",
    "Note that we use keywords in the input like `Context:`, `Question:` to separate the text representing the context and question. We also use the keyword `User:` and end each of the input with `\\n\\nAssistant:` tokens. These are recommended because NeMo's instruction-tuned models are trained with a prefix of `User:` and suffix `\\n\\nAssistant:`.\n",
    "\n",
    "The SQuAD dataset does not already reflect this, so let's go ahead and preprocess it to fit the above format. \n",
    "\n",
    "To do so, a processing script has been included with this project template. Feel free to take a look inside the `prompt_learning_squad_preprocessing.py` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c06ea9b1-7546-4125-8e5e-48bc8d491c51",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving train split to /mnt/code/data/SQuAD/squad_train.jsonl\n",
      "100%|█████████████████████████████████| 87599/87599 [00:00<00:00, 173631.87it/s]\n",
      "Saving val split to /mnt/code/data/SQuAD/squad_val.jsonl\n",
      "100%|█████████████████████████████████| 10570/10570 [00:00<00:00, 170808.25it/s]\n",
      "Saving test split to /mnt/code/data/SQuAD/squad_test_ground_truth.jsonl\n",
      "100%|█████████████████████████████████| 10570/10570 [00:00<00:00, 157694.62it/s]\n",
      "Saving test split to /mnt/code/data/SQuAD/squad_test.jsonl\n",
      "100%|█████████████████████████████████| 10570/10570 [00:00<00:00, 169686.39it/s]\n"
     ]
    }
   ],
   "source": [
    "# Preprocess squad data\n",
    "!python /opt/NeMo/scripts/dataset_processing/nlp/squad/prompt_learning_squad_preprocessing.py --sft-format --data-dir {SQUAD_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cbbe87-b081-4e2d-bb4a-58ca1536a3d4",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's split the datasets into train and validation files, and take a look at a few samples of the data to confirm the preprocessing is satisfactory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9558dca-2ee2-43ec-a64f-9708d5cd6889",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"input\": \"User: Context:Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24\\u201310 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \\\"golden anniversary\\\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \\\"Super Bowl L\\\"), so that the logo could prominently feature the Arabic numerals 50. Question:Which NFL team represented the AFC at Super Bowl 50?\\n\\nAssistant:\", \"output\": \"Denver Broncos\"}\n",
      "{\"input\": \"User: Context:Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24\\u201310 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \\\"golden anniversary\\\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \\\"Super Bowl L\\\"), so that the logo could prominently feature the Arabic numerals 50. Question:Which NFL team represented the NFC at Super Bowl 50?\\n\\nAssistant:\", \"output\": \"Carolina Panthers\"}\n",
      "{\"input\": \"User: Context:Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24\\u201310 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \\\"golden anniversary\\\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \\\"Super Bowl L\\\"), so that the logo could prominently feature the Arabic numerals 50. Question:Where did Super Bowl 50 take place?\\n\\nAssistant:\", \"output\": \"Santa Clara, California\"}\n",
      "{\"input\": \"User: Context:Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24\\u201310 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \\\"golden anniversary\\\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \\\"Super Bowl L\\\"), so that the logo could prominently feature the Arabic numerals 50. Question:Which NFL team won Super Bowl 50?\\n\\nAssistant:\", \"output\": \"Denver Broncos\"}\n",
      "{\"input\": \"User: Context:Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \\\"Venite Ad Me Omnes\\\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary. Question:To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\\n\\nAssistant:\", \"output\": \"Saint Bernadette Soubirous\"}\n",
      "{\"input\": \"User: Context:Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \\\"Venite Ad Me Omnes\\\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary. Question:What is in front of the Notre Dame Main Building?\\n\\nAssistant:\", \"output\": \"a copper statue of Christ\"}\n",
      "{\"input\": \"User: Context:Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \\\"Venite Ad Me Omnes\\\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary. Question:The Basilica of the Sacred heart at Notre Dame is beside to which structure?\\n\\nAssistant:\", \"output\": \"the Main Building\"}\n",
      "{\"input\": \"User: Context:Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \\\"Venite Ad Me Omnes\\\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary. Question:What is the Grotto at Notre Dame?\\n\\nAssistant:\", \"output\": \"a Marian place of prayer and reflection\"}\n"
     ]
    }
   ],
   "source": [
    "# What the squad dataset looks like after processing\n",
    "! head -1000 $SQUAD_DIR/squad_train.jsonl > $SQUAD_DIR/squad_short_train.jsonl\n",
    "! head -25 $SQUAD_DIR/squad_val.jsonl > $SQUAD_DIR/squad_short_val.jsonl\n",
    "! head -4 $SQUAD_DIR/squad_short_val.jsonl\n",
    "! head -4 $SQUAD_DIR/squad_short_train.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5327710d-3835-41ec-a1a4-49fccc5dd271",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Now that the model is available and the data is prepared, we are ready to start the training.\n",
    "\n",
    "### Load Config\n",
    "\n",
    "The NeMo toolkit leverages a configuration file to make it easy to define and explore with training parameters without having to change the code. For this project template, a default configuration for fine-tuning has been included at `conf/nemotron-finetune-config.yaml` which is based off the sample configs provided by NVIDIA here: https://github.com/NVIDIA/NeMo/tree/main/examples/nlp/language_modeling/tuning/conf\n",
    "\n",
    "We will start by loading in that configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32b37b20-3625-4543-94fd-3530537c1314",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "\n",
    "cfg = OmegaConf.load(\"/mnt/code/conf/nemotron-finetune-config.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecc6db8-8e5a-4b5e-92c6-67902d968a3d",
   "metadata": {},
   "source": [
    "With the config loaded, we can override certain settings for our environment. The default values should work but here are some parameter that you may want to adjust:\n",
    "\n",
    "* `config.trainer.precision` - This is the precision that will be used during fine-tuning. The model might be more accurate with higher values but it also uses more memory than lower precisions. If the fine-tuning process runs out of memory, try reducing the precision here.\n",
    "* `config.trainer.devices` - This is the number of devices that will be used. If running on a multi-GPU system, increase this number as appropriate.\n",
    "* `config.model.global_batch_size` - If using a higher GPU count or if additional GPU memory allows, this value can be increased for higher performance. Note that higher batch sizes use more GPU memory.\n",
    "\n",
    "One config that you will want to update is the `config.model.restore_from_path`. This should point to the `.nemo` file where your model is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9f0a870-40e2-421a-a88d-f78f730d357c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cfg.model.restore_from_path=MODEL_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510647e8-c07f-432f-a3ce-766c37e74edb",
   "metadata": {},
   "source": [
    "By default, this notebook doesn't use distributed training so we will set some environment variables accordingly. If you do choose to use distributed training methods, you may want to change the environment variables below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86d3f1a6-256b-4b87-855b-0fef507bec05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"LOCAL_RANK\"] = '0'\n",
    "os.environ[\"RANK\"] = '0'\n",
    "os.environ[\"WORLD_SIZE\"] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f614b4d-c40d-4c80-a986-fffafd96a5e7",
   "metadata": {},
   "source": [
    "### Configure Training\n",
    "\n",
    "We now load in our model and configure the trainer using the loaded config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d48817d-fb61-471a-b6e7-7591433288a6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-03-20 15:32:20 megatron_trainer_builder:51] Detected interactive environment, using NLPDDPStrategyNotebook\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "[NeMo W 2024-03-20 15:32:33 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:32:33 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:32:33 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:32:33 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:32:33 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:32:33 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:32:33 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:32:33 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:32:33 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:32:33 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:32:33 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:32:33 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:32:33 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:32:33 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:32:33 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:32:33 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:32:33 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:32:33 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:32:33 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-03-20 15:32:33 megatron_init:241] Rank 0 has data parallel group : [0]\n",
      "[NeMo I 2024-03-20 15:32:33 megatron_init:247] Rank 0 has combined group of data parallel and context parallel : [0]\n",
      "[NeMo I 2024-03-20 15:32:33 megatron_init:252] All data parallel group ranks with context parallel combined: [[0]]\n",
      "[NeMo I 2024-03-20 15:32:33 megatron_init:255] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2024-03-20 15:32:33 megatron_init:272] Rank 0 has context parallel group: [0]\n",
      "[NeMo I 2024-03-20 15:32:33 megatron_init:275] All context parallel group ranks: [[0]]\n",
      "[NeMo I 2024-03-20 15:32:33 megatron_init:276] Ranks 0 has context parallel rank: 0\n",
      "[NeMo I 2024-03-20 15:32:33 megatron_init:287] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2024-03-20 15:32:33 megatron_init:288] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-03-20 15:32:33 megatron_init:298] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2024-03-20 15:32:33 megatron_init:302] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-03-20 15:32:33 megatron_init:303] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2024-03-20 15:32:33 megatron_init:317] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2024-03-20 15:32:33 megatron_init:329] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2024-03-20 15:32:33 megatron_init:335] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-03-20 15:32:33 megatron_init:336] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2024-03-20 15:32:33 megatron_init:337] All embedding group ranks: [[0]]\n",
      "[NeMo I 2024-03-20 15:32:33 megatron_init:338] Rank 0 has embedding rank: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[NeMo W 2024-03-20 15:32:33 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:32:33 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:32:33 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:32:33 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:32:33 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:32:33 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:32:33 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:32:33 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:32:33 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:32:33 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:32:33 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:32:33 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:32:33 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:32:33 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:32:33 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:32:33 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:32:33 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:32:33 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:32:33 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-03-20 15:32:33 tokenizer_utils:191] Getting SentencePiece with model: /tmp/tmpb0n9acnj/586f3f51a9cf43bc9369bd53fa08868c_a934dc7c3e1e46a6838bb63379916563_3feba89c944047c19d5a1d0c07a85c32_mt_nlg_plus_multilingual_ja_zh_the_stack_frac_015_256k.model\n",
      "[NeMo I 2024-03-20 15:32:33 megatron_base_model:539] Padded vocab_size: 256000, original vocab_size: 256000, dummy tokens: 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-03-20 15:32:33 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:32:33 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:32:33 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:32:33 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:32:33 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:32:33 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:32:33 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:32:33 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:32:33 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:32:33 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:32:33 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:32:33 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:32:33 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:32:33 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:32:33 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:32:33 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:32:33 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:32:33 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:32:33 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:32:33 megatron_base_model:455] apply_query_key_layer_scaling is only enabled when using FP16, setting it to False and setting NVTE_APPLY_QK_LAYER_SCALING=0\n",
      "[NeMo W 2024-03-20 15:32:33 megatron_base_model:511] The model: MegatronGPTSFTModel() does not have field.name: num_query_groups in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:32:33 megatron_base_model:511] The model: MegatronGPTSFTModel() does not have field.name: add_qkv_bias in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:32:33 megatron_base_model:511] The model: MegatronGPTSFTModel() does not have field.name: num_moe_experts in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:32:33 megatron_base_model:511] The model: MegatronGPTSFTModel() does not have field.name: rotary_interleaved in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:32:33 megatron_base_model:511] The model: MegatronGPTSFTModel() does not have field.name: window_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:32:33 megatron_base_model:511] The model: MegatronGPTSFTModel() does not have field.name: memory_efficient_layer_norm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:32:33 megatron_base_model:511] The model: MegatronGPTSFTModel() does not have field.name: fp8_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:32:33 megatron_base_model:511] The model: MegatronGPTSFTModel() does not have field.name: clone_scatter_output_in_embedding in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:32:33 megatron_base_model:511] The model: MegatronGPTSFTModel() does not have field.name: moe_router_load_balancing_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:32:33 megatron_base_model:511] The model: MegatronGPTSFTModel() does not have field.name: moe_router_topk in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:32:33 megatron_base_model:511] The model: MegatronGPTSFTModel() does not have field.name: moe_grouped_gemm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:32:33 megatron_base_model:511] The model: MegatronGPTSFTModel() does not have field.name: moe_aux_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:32:33 megatron_base_model:511] The model: MegatronGPTSFTModel() does not have field.name: moe_z_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:32:33 megatron_base_model:511] The model: MegatronGPTSFTModel() does not have field.name: moe_input_jitter_eps in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:32:33 megatron_base_model:511] The model: MegatronGPTSFTModel() does not have field.name: moe_token_dropping in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading distributed checkpoint with TensorStoreLoadShardedStrategy\n",
      "[NeMo I 2024-03-20 15:34:26 nlp_overrides:1108] Model MegatronGPTSFTModel was successfully restored from /mnt/artifacts/nemotron/Nemotron-3-8B-Base-4k.nemo.\n",
      "[NeMo I 2024-03-20 15:34:26 nlp_adapter_mixins:184] Before adding PEFT params:\n",
      "      | Name  | Type     | Params\n",
      "    -----------------------------------\n",
      "    0 | model | GPTModel | 8.5 B \n",
      "    -----------------------------------\n",
      "    0         Trainable params\n",
      "    8.5 B     Non-trainable params\n",
      "    8.5 B     Total params\n",
      "    34,160.542Total estimated model params size (MB)\n",
      "[NeMo I 2024-03-20 15:34:28 nlp_adapter_mixins:197] After adding PEFT params:\n",
      "      | Name  | Type     | Params\n",
      "    -----------------------------------\n",
      "    0 | model | GPTModel | 8.6 B \n",
      "    -----------------------------------\n",
      "    16.8 M    Trainable params\n",
      "    8.5 B     Non-trainable params\n",
      "    8.6 B     Total params\n",
      "    34,227.651Total estimated model params size (MB)\n"
     ]
    }
   ],
   "source": [
    "from nemo.collections.nlp.parts.megatron_trainer_builder import MegatronLMPPTrainerBuilder\n",
    "from nemo.collections.nlp.models.language_modeling.megatron_gpt_sft_model import MegatronGPTSFTModel\n",
    "from nemo.collections.nlp.parts.peft_config import LoraPEFTConfig\n",
    "\n",
    "trainer = MegatronLMPPTrainerBuilder(cfg).create_trainer()\n",
    "model_cfg = MegatronGPTSFTModel.merge_cfg_with(cfg.model.restore_from_path, cfg)\n",
    "model = MegatronGPTSFTModel.restore_from(cfg.model.restore_from_path, model_cfg, trainer=trainer)\n",
    "model.add_adapter(LoraPEFTConfig(model_cfg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ab9e65-04a7-4bc9-9460-af092ff1d482",
   "metadata": {},
   "source": [
    "### Configure experiment\n",
    "We will also activate the experiment logging so that we can create checkpoints to resume from later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db58b2b4-434f-4354-a985-91a1b3926816",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-03-20 15:34:28 exp_manager:759] No version folders would be created under the log folder as 'resume_if_exists' is enabled.\n",
      "[NeMo W 2024-03-20 15:34:28 exp_manager:616] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :/mnt/code/nemo_experiments/megatron_gpt_peft_lora_tuning/checkpoints. Training from scratch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-03-20 15:34:28 exp_manager:396] Experiments will be logged at /mnt/code/nemo_experiments/megatron_gpt_peft_lora_tuning\n",
      "[NeMo I 2024-03-20 15:34:28 exp_manager:842] TensorboardLogger has been set up\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-03-20 15:34:28 exp_manager:952] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to 100. Please ensure that max_steps will run for at least 1 epochs to ensure that checkpointing will not error out.\n"
     ]
    }
   ],
   "source": [
    "from nemo.utils.exp_manager import exp_manager\n",
    "\n",
    "exp_dir = exp_manager(trainer, cfg.get(\"exp_manager\", None))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1754849-1b32-49ce-a019-7731e547673f",
   "metadata": {},
   "source": [
    "### Train model\n",
    "Lastly, we can finally train our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9e9c552-fa12-460b-9db7-a41722397922",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-03-20 15:34:28 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:153: UserWarning: The `batch_idx` argument in `MegatronGPTSFTModel.on_train_batch_start` hook may not match with the actual batch index when using a `dataloader_iter` argument in your `training_step`.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "[NeMo W 2024-03-20 15:34:28 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:153: UserWarning: The `batch_idx` argument in `MegatronGPTSFTModel.on_train_batch_end` hook may not match with the actual batch index when using a `dataloader_iter` argument in your `training_step`.\n",
      "      rank_zero_warn(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-03-20 15:34:28 megatron_gpt_sft_model:767] Building GPT SFT validation datasets.\n",
      "[NeMo I 2024-03-20 15:34:28 text_memmap_dataset:116] Building data files\n",
      "[NeMo I 2024-03-20 15:34:28 text_memmap_dataset:525] Processing 1 data files using 2 workers\n",
      "[NeMo I 2024-03-20 15:34:28 text_memmap_dataset:495] Building indexing for fn = /mnt/code/data/SQuAD/squad_short_val.jsonl\n",
      "[NeMo I 2024-03-20 15:34:28 text_memmap_dataset:507] Saving idx file = /mnt/code/data/SQuAD/squad_short_val.jsonl.idx.npy\n",
      "[NeMo I 2024-03-20 15:34:28 text_memmap_dataset:509] Saving metadata file = /mnt/code/data/SQuAD/squad_short_val.jsonl.idx.info\n",
      "[NeMo I 2024-03-20 15:34:28 text_memmap_dataset:535] Time building 1 / 1 mem-mapped files: 0:00:00.100736\n",
      "[NeMo I 2024-03-20 15:34:28 text_memmap_dataset:525] Processing 1 data files using 2 workers\n",
      "[NeMo I 2024-03-20 15:34:29 text_memmap_dataset:535] Time building 0 / 1 mem-mapped files: 0:00:00.088718\n",
      "[NeMo I 2024-03-20 15:34:29 text_memmap_dataset:158] Loading data files\n",
      "[NeMo I 2024-03-20 15:34:29 text_memmap_dataset:249] Loading /mnt/code/data/SQuAD/squad_short_val.jsonl\n",
      "[NeMo I 2024-03-20 15:34:29 text_memmap_dataset:161] Time loading 1 mem-mapped files: 0:00:00.001422\n",
      "[NeMo I 2024-03-20 15:34:29 text_memmap_dataset:165] Computing global indices\n",
      "[NeMo I 2024-03-20 15:34:29 megatron_gpt_sft_model:770] Length of val dataset: 25\n",
      "[NeMo I 2024-03-20 15:34:29 megatron_gpt_sft_model:774] Building GPT SFT test datasets.\n",
      "[NeMo I 2024-03-20 15:34:29 text_memmap_dataset:116] Building data files\n",
      "[NeMo I 2024-03-20 15:34:29 text_memmap_dataset:525] Processing 1 data files using 2 workers\n",
      "[NeMo I 2024-03-20 15:34:29 text_memmap_dataset:535] Time building 0 / 1 mem-mapped files: 0:00:00.085571\n",
      "[NeMo I 2024-03-20 15:34:29 text_memmap_dataset:525] Processing 1 data files using 2 workers\n",
      "[NeMo I 2024-03-20 15:34:29 text_memmap_dataset:535] Time building 0 / 1 mem-mapped files: 0:00:00.087091\n",
      "[NeMo I 2024-03-20 15:34:29 text_memmap_dataset:158] Loading data files\n",
      "[NeMo I 2024-03-20 15:34:29 text_memmap_dataset:249] Loading /mnt/code/data/SQuAD/squad_short_val.jsonl\n",
      "[NeMo I 2024-03-20 15:34:29 text_memmap_dataset:161] Time loading 1 mem-mapped files: 0:00:00.001157\n",
      "[NeMo I 2024-03-20 15:34:29 text_memmap_dataset:165] Computing global indices\n",
      "[NeMo I 2024-03-20 15:34:29 megatron_gpt_sft_model:777] Length of test dataset: 25\n",
      "[NeMo I 2024-03-20 15:34:29 megatron_gpt_sft_model:781] Building GPT SFT traing datasets.\n",
      "[NeMo I 2024-03-20 15:34:29 text_memmap_dataset:116] Building data files\n",
      "[NeMo I 2024-03-20 15:34:29 text_memmap_dataset:525] Processing 1 data files using 2 workers\n",
      "[NeMo I 2024-03-20 15:34:29 text_memmap_dataset:495] Building indexing for fn = /mnt/code/data/SQuAD/squad_short_train.jsonl\n",
      "[NeMo I 2024-03-20 15:34:29 text_memmap_dataset:507] Saving idx file = /mnt/code/data/SQuAD/squad_short_train.jsonl.idx.npy\n",
      "[NeMo I 2024-03-20 15:34:29 text_memmap_dataset:509] Saving metadata file = /mnt/code/data/SQuAD/squad_short_train.jsonl.idx.info\n",
      "[NeMo I 2024-03-20 15:34:29 text_memmap_dataset:535] Time building 1 / 1 mem-mapped files: 0:00:00.090769\n",
      "[NeMo I 2024-03-20 15:34:29 text_memmap_dataset:525] Processing 1 data files using 2 workers\n",
      "[NeMo I 2024-03-20 15:34:29 text_memmap_dataset:535] Time building 0 / 1 mem-mapped files: 0:00:00.093739\n",
      "[NeMo I 2024-03-20 15:34:29 text_memmap_dataset:158] Loading data files\n",
      "[NeMo I 2024-03-20 15:34:29 text_memmap_dataset:249] Loading /mnt/code/data/SQuAD/squad_short_train.jsonl\n",
      "[NeMo I 2024-03-20 15:34:29 text_memmap_dataset:161] Time loading 1 mem-mapped files: 0:00:00.001296\n",
      "[NeMo I 2024-03-20 15:34:29 text_memmap_dataset:165] Computing global indices\n",
      " > WARNING: could not find index map file /mnt/code/data/SQuAD/squad_short_train.jsonl_squad_short_train.jsonl_indexmap_101mns_2046msl_0.00ssp_1234s.npy, building the indices on rank 0 ...\n",
      "[NeMo I 2024-03-20 15:34:29 dataset_utils:1303]  > building samples index mapping for squad_short_train.jsonl ...\n",
      "make: Entering directory '/opt/NeMo/nemo/collections/nlp/data/language_modeling/megatron'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/opt/NeMo/nemo/collections/nlp/data/language_modeling/megatron'\n",
      "[NeMo I 2024-03-20 15:34:29 dataset_utils:1324]  > done building samples index maping\n",
      "    using uint32 for data mapping...\n",
      "    using:\n",
      "     number of documents:            1000\n",
      "     sentences range:                [0, 1000)\n",
      "     total number of sentences:      1000\n",
      "     number of epochs:               2147483646\n",
      "     maximum number of samples:      101\n",
      "     maximum sequence length:        2046\n",
      "     short sequence probability:     0\n",
      "     short sequence ration (1/prob): 0\n",
      "     seed:                           1234\n",
      "    reached 101 samples after 1 epochs ...\n",
      "   number of empty documents: 0\n",
      "   number of documents with one sentence: 1000\n",
      "   number of documents with long sentences: 0\n",
      "   will create mapping for 1000 samples\n",
      "[NeMo I 2024-03-20 15:34:29 dataset_utils:1326]  > saved the index mapping in /mnt/code/data/SQuAD/squad_short_train.jsonl_squad_short_train.jsonl_indexmap_101mns_2046msl_0.00ssp_1234s.npy\n",
      "[NeMo I 2024-03-20 15:34:29 dataset_utils:1328]  > elasped time to build and save samples mapping (seconds): 0.044665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-03-20 15:34:29 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/data/language_modeling/megatron/dataset_utils.py:1332: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "      counts = torch.cuda.LongTensor([1])\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make: Entering directory '/opt/NeMo/nemo/collections/nlp/data/language_modeling/megatron'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/opt/NeMo/nemo/collections/nlp/data/language_modeling/megatron'\n",
      "[NeMo I 2024-03-20 15:34:29 blendable_dataset:67] > elapsed time for building blendable dataset indices: 0.04 (sec)\n",
      "> building indices for blendable datasets ...\n",
      " > sample ratios:\n",
      "   dataset 0, input: 1, achieved: 1\n",
      "[NeMo I 2024-03-20 15:34:29 megatron_gpt_sft_model:783] Length of train dataset: 101\n",
      "[NeMo I 2024-03-20 15:34:29 megatron_gpt_sft_model:788] Building dataloader with consumed samples: 0\n",
      "[NeMo I 2024-03-20 15:34:29 megatron_gpt_sft_model:788] Building dataloader with consumed samples: 0\n",
      "[NeMo I 2024-03-20 15:34:29 megatron_gpt_sft_model:788] Building dataloader with consumed samples: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-03-20 15:34:29 nlp_overrides:227] Configuring DDP for model parallelism.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-03-20 15:34:29 megatron_base_model:1145] Ignoring `trainer.max_epochs` when computing `max_steps` because `trainer.max_steps` is already set to 100.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-03-20 15:34:29 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-03-20 15:34:29 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-03-20 15:34:29 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-03-20 15:34:29 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-03-20 15:34:29 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-03-20 15:34:29 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-03-20 15:34:29 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-03-20 15:34:29 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-03-20 15:34:29 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-03-20 15:34:29 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-03-20 15:34:29 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-03-20 15:34:29 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-03-20 15:34:29 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-03-20 15:34:29 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-03-20 15:34:29 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-03-20 15:34:29 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-03-20 15:34:29 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-03-20 15:34:29 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-03-20 15:34:29 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-03-20 15:34:29 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-03-20 15:34:29 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-03-20 15:34:29 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-03-20 15:34:29 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-03-20 15:34:29 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-03-20 15:34:29 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-03-20 15:34:29 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-03-20 15:34:29 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-03-20 15:34:29 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-03-20 15:34:29 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-03-20 15:34:29 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-03-20 15:34:29 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-03-20 15:34:29 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-03-20 15:34:29 nlp_adapter_mixins:256] Optimizer groups set:\n",
      "      | Name  | Type     | Params\n",
      "    -----------------------------------\n",
      "    0 | model | GPTModel | 8.6 B \n",
      "    -----------------------------------\n",
      "    16.8 M    Trainable params\n",
      "    8.5 B     Non-trainable params\n",
      "    8.6 B     Total params\n",
      "    34,227.651Total estimated model params size (MB)\n",
      "[NeMo I 2024-03-20 15:34:29 modelPT:723] Optimizer config = FusedAdam (\n",
      "    Parameter Group 0\n",
      "        betas: [0.9, 0.98]\n",
      "        bias_correction: True\n",
      "        eps: 1e-08\n",
      "        lr: 0.0001\n",
      "        weight_decay: 0.01\n",
      "    )\n",
      "[NeMo I 2024-03-20 15:34:29 lr_scheduler:915] Scheduler \"<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x7ffe0846d7b0>\" \n",
      "    will be used during training (effective maximum steps = 100) - \n",
      "    Parameters : \n",
      "    (warmup_steps: 50\n",
      "    min_lr: 0.0\n",
      "    constant_steps: 0\n",
      "    max_steps: 100\n",
      "    )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type     | Params\n",
      "-----------------------------------\n",
      "0 | model | GPTModel | 8.6 B \n",
      "-----------------------------------\n",
      "16.8 M    Trainable params\n",
      "8.5 B     Non-trainable params\n",
      "8.6 B     Total params\n",
      "34,227.651Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-03-20 15:34:30 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:438: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "[NeMo W 2024-03-20 15:34:30 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:148: UserWarning: Found `dataloader_iter` argument in the `validation_step`. Note that the support for this signature is experimental and the behavior is subject to change.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "[NeMo W 2024-03-20 15:34:30 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/apex/transformer/pipeline_parallel/utils.py:81: UserWarning: This function is only for unittest\n",
      "      warnings.warn(\"This function is only for unittest\")\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: : 3it [00:01,  2.78it/s]                     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-03-20 15:34:31 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:433: PossibleUserWarning: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "      warning_cache.warn(\n",
      "    \n",
      "[NeMo W 2024-03-20 15:34:31 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:433: PossibleUserWarning: It is recommended to use `self.log('validation_loss_squad_val', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "      warning_cache.warn(\n",
      "    \n",
      "[NeMo W 2024-03-20 15:34:31 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:433: PossibleUserWarning: It is recommended to use `self.log('validation_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "      warning_cache.warn(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-03-20 15:34:31 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:148: UserWarning: Found `dataloader_iter` argument in the `training_step`. Note that the support for this signature is experimental and the behavior is subject to change.\n",
      "      rank_zero_warn(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: :   0%|          | 0/101 [00:00<?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-03-20 15:34:32 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('global_step', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.\n",
      "      warning_cache.warn(\n",
      "    \n",
      "[NeMo W 2024-03-20 15:34:32 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('consumed_samples', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.\n",
      "      warning_cache.warn(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: :  10%|▉         | 10/101 [00:02<00:19, v_num=0, reduced_train_loss=3.700, global_step=9.000, consumed_samples=10.00, train_step_timing in s=0.133]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▍         | 1/25 [00:00<00:03,  7.39it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|▊         | 2/25 [00:00<00:02,  8.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|█▏        | 3/25 [00:00<00:02,  9.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|█▌        | 4/25 [00:00<00:04,  4.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|██        | 5/25 [00:00<00:03,  5.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|██▍       | 6/25 [00:01<00:03,  5.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|██▊       | 7/25 [00:01<00:02,  6.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|███▏      | 8/25 [00:01<00:03,  4.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|███▌      | 9/25 [00:01<00:03,  5.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|████      | 10/25 [00:01<00:02,  5.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|████▍     | 11/25 [00:01<00:02,  5.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████▊     | 12/25 [00:02<00:02,  5.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|█████▏    | 13/25 [00:02<00:01,  6.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████▌    | 14/25 [00:02<00:01,  6.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|██████    | 15/25 [00:02<00:01,  6.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████▍   | 16/25 [00:02<00:01,  6.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████▊   | 17/25 [00:02<00:01,  6.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|███████▏  | 18/25 [00:02<00:01,  6.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|███████▌  | 19/25 [00:02<00:00,  6.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████  | 20/25 [00:02<00:00,  7.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|████████▍ | 21/25 [00:02<00:00,  7.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|████████▊ | 22/25 [00:03<00:00,  7.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|█████████▏| 23/25 [00:03<00:00,  7.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|█████████▌| 24/25 [00:03<00:00,  7.50it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 25/25 [00:03<00:00,  7.64it/s]\u001b[A\n",
      "Epoch 0: :  10%|▉         | 10/101 [00:05<00:48, v_num=0, reduced_train_loss=3.700, global_step=9.000, consumed_samples=10.00, train_step_timing in s=0.133, val_loss=1.600]\n",
      "                                                  \u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 10: 'validation_loss' reached 1.60287 (best 1.60287), saving model to '/mnt/code/nemo_experiments/megatron_gpt_peft_lora_tuning/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=1.603-step=10-consumed_samples=10.0.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: :  20%|█▉        | 20/101 [00:07<00:29, v_num=0, reduced_train_loss=0.267, global_step=19.00, consumed_samples=20.00, train_step_timing in s=0.153, val_loss=1.600]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▍         | 1/25 [00:00<00:01, 12.03it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|▊         | 2/25 [00:00<00:01, 12.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|█▏        | 3/25 [00:00<00:01, 12.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|█▌        | 4/25 [00:00<00:01, 12.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|██        | 5/25 [00:00<00:01, 11.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|██▍       | 6/25 [00:00<00:02,  9.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|██▊       | 7/25 [00:00<00:01,  9.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|███▏      | 8/25 [00:00<00:01,  9.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|███▌      | 9/25 [00:00<00:01, 10.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|████      | 10/25 [00:00<00:01, 10.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|████▍     | 11/25 [00:01<00:01, 10.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████▊     | 12/25 [00:01<00:01, 10.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|█████▏    | 13/25 [00:01<00:01, 10.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████▌    | 14/25 [00:01<00:01, 10.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|██████    | 15/25 [00:01<00:00, 10.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████▍   | 16/25 [00:01<00:00, 10.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████▊   | 17/25 [00:01<00:00, 10.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|███████▏  | 18/25 [00:01<00:00,  9.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|███████▌  | 19/25 [00:01<00:00,  9.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████  | 20/25 [00:02<00:00,  9.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|████████▍ | 21/25 [00:02<00:00,  9.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|████████▊ | 22/25 [00:02<00:00,  9.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|█████████▏| 23/25 [00:02<00:00, 10.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|█████████▌| 24/25 [00:02<00:00, 10.14it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 25/25 [00:02<00:00, 10.23it/s]\u001b[A\n",
      "Epoch 0: :  20%|█▉        | 20/101 [00:09<00:39, v_num=0, reduced_train_loss=0.267, global_step=19.00, consumed_samples=20.00, train_step_timing in s=0.153, val_loss=0.492]\n",
      "                                                  \u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 20: 'validation_loss' reached 0.49183 (best 0.49183), saving model to '/mnt/code/nemo_experiments/megatron_gpt_peft_lora_tuning/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=0.492-step=20-consumed_samples=20.0.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-03-20 15:34:41 nlp_overrides:463] Removing checkpoint: /mnt/code/nemo_experiments/megatron_gpt_peft_lora_tuning/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=1.603-step=10-consumed_samples=10.0.ckpt\n",
      "[NeMo I 2024-03-20 15:34:42 nlp_overrides:463] Removing checkpoint: /mnt/code/nemo_experiments/megatron_gpt_peft_lora_tuning/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=1.603-step=10-consumed_samples=10.0-last.ckpt\n",
      "Epoch 0: :  30%|██▉       | 30/101 [00:11<00:27, v_num=0, reduced_train_loss=0.807, global_step=29.00, consumed_samples=30.00, train_step_timing in s=0.148, val_loss=0.492]  \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▍         | 1/25 [00:00<00:03,  6.98it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|▊         | 2/25 [00:00<00:02,  9.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|█▏        | 3/25 [00:00<00:02,  9.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|█▌        | 4/25 [00:00<00:02, 10.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|██        | 5/25 [00:00<00:01, 10.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|██▍       | 6/25 [00:00<00:01, 10.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|██▊       | 7/25 [00:00<00:01, 10.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|███▏      | 8/25 [00:00<00:01, 10.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|███▌      | 9/25 [00:00<00:01,  9.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|████      | 10/25 [00:01<00:01,  9.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|████▍     | 11/25 [00:01<00:01,  9.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████▊     | 12/25 [00:01<00:01,  9.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|█████▏    | 13/25 [00:01<00:01,  9.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████▌    | 14/25 [00:01<00:01,  9.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|██████    | 15/25 [00:01<00:01,  9.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████▍   | 16/25 [00:01<00:00,  9.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████▊   | 17/25 [00:01<00:00,  9.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|███████▏  | 18/25 [00:01<00:00,  9.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|███████▌  | 19/25 [00:01<00:00,  9.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████  | 20/25 [00:02<00:00,  9.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|████████▍ | 21/25 [00:02<00:00,  9.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|████████▊ | 22/25 [00:02<00:00,  9.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|█████████▏| 23/25 [00:02<00:00,  9.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|█████████▌| 24/25 [00:02<00:00,  9.86it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 25/25 [00:02<00:00,  9.86it/s]\u001b[A\n",
      "Epoch 0: :  30%|██▉       | 30/101 [00:14<00:33, v_num=0, reduced_train_loss=0.807, global_step=29.00, consumed_samples=30.00, train_step_timing in s=0.148, val_loss=0.215]\n",
      "                                                  \u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 30: 'validation_loss' reached 0.21462 (best 0.21462), saving model to '/mnt/code/nemo_experiments/megatron_gpt_peft_lora_tuning/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=0.215-step=30-consumed_samples=30.0.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-03-20 15:34:46 nlp_overrides:463] Removing checkpoint: /mnt/code/nemo_experiments/megatron_gpt_peft_lora_tuning/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=0.492-step=20-consumed_samples=20.0.ckpt\n",
      "[NeMo I 2024-03-20 15:34:46 nlp_overrides:463] Removing checkpoint: /mnt/code/nemo_experiments/megatron_gpt_peft_lora_tuning/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=0.492-step=20-consumed_samples=20.0-last.ckpt\n",
      "Epoch 0: :  40%|███▉      | 40/101 [00:16<00:24, v_num=0, reduced_train_loss=0.0501, global_step=39.00, consumed_samples=40.00, train_step_timing in s=0.127, val_loss=0.215]  \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▍         | 1/25 [00:00<00:01, 12.47it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|▊         | 2/25 [00:00<00:02, 11.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|█▏        | 3/25 [00:00<00:01, 11.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|█▌        | 4/25 [00:00<00:02,  8.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|██        | 5/25 [00:00<00:02,  8.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|██▍       | 6/25 [00:00<00:02,  8.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|██▊       | 7/25 [00:00<00:01,  9.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|███▏      | 8/25 [00:00<00:01,  9.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|███▌      | 9/25 [00:00<00:01,  9.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|████      | 10/25 [00:01<00:01,  9.25it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|████▍     | 11/25 [00:01<00:01,  9.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████▊     | 12/25 [00:01<00:01,  8.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|█████▏    | 13/25 [00:01<00:01,  8.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████▌    | 14/25 [00:01<00:01,  8.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|██████    | 15/25 [00:01<00:01,  8.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████▍   | 16/25 [00:01<00:01,  8.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████▊   | 17/25 [00:01<00:00,  8.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|███████▏  | 18/25 [00:02<00:00,  8.24it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|███████▌  | 19/25 [00:02<00:00,  8.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████  | 20/25 [00:02<00:00,  8.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|████████▍ | 21/25 [00:02<00:00,  8.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|████████▊ | 22/25 [00:02<00:00,  8.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|█████████▏| 23/25 [00:02<00:00,  8.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|█████████▌| 24/25 [00:02<00:00,  8.67it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 25/25 [00:02<00:00,  8.74it/s]\u001b[A\n",
      "Epoch 0: :  40%|███▉      | 40/101 [00:19<00:29, v_num=0, reduced_train_loss=0.0501, global_step=39.00, consumed_samples=40.00, train_step_timing in s=0.127, val_loss=0.281]\n",
      "                                                  \u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 40: 'validation_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-03-20 15:34:51 nlp_overrides:463] Removing checkpoint: /mnt/code/nemo_experiments/megatron_gpt_peft_lora_tuning/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=0.215-step=30-consumed_samples=30.0-last.ckpt\n",
      "Epoch 0: :  50%|████▉     | 50/101 [00:20<00:21, v_num=0, reduced_train_loss=0.026, global_step=49.00, consumed_samples=50.00, train_step_timing in s=0.128, val_loss=0.281]  \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▍         | 1/25 [00:00<00:01, 12.36it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|▊         | 2/25 [00:00<00:02, 11.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|█▏        | 3/25 [00:00<00:03,  6.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|█▌        | 4/25 [00:00<00:03,  6.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|██        | 5/25 [00:00<00:02,  7.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|██▍       | 6/25 [00:00<00:02,  7.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|██▊       | 7/25 [00:00<00:02,  8.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|███▏      | 8/25 [00:00<00:01,  8.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|███▌      | 9/25 [00:00<00:01,  9.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|████      | 10/25 [00:01<00:01,  9.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|████▍     | 11/25 [00:01<00:01,  9.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████▊     | 12/25 [00:01<00:01,  8.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|█████▏    | 13/25 [00:01<00:01,  8.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████▌    | 14/25 [00:01<00:01,  8.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|██████    | 15/25 [00:01<00:01,  8.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████▍   | 16/25 [00:01<00:00,  9.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████▊   | 17/25 [00:01<00:00,  9.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|███████▏  | 18/25 [00:01<00:00,  9.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|███████▌  | 19/25 [00:01<00:00,  9.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████  | 20/25 [00:02<00:00,  9.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|████████▍ | 21/25 [00:02<00:00,  9.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|████████▊ | 22/25 [00:02<00:00,  9.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|█████████▏| 23/25 [00:02<00:00,  9.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|█████████▌| 24/25 [00:02<00:00,  9.69it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 25/25 [00:02<00:00,  9.36it/s]\u001b[A\n",
      "Epoch 0: :  50%|████▉     | 50/101 [00:23<00:23, v_num=0, reduced_train_loss=0.026, global_step=49.00, consumed_samples=50.00, train_step_timing in s=0.128, val_loss=0.157]\n",
      "                                                  \u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 50: 'validation_loss' reached 0.15708 (best 0.15708), saving model to '/mnt/code/nemo_experiments/megatron_gpt_peft_lora_tuning/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=0.157-step=50-consumed_samples=50.0.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-03-20 15:34:55 nlp_overrides:463] Removing checkpoint: /mnt/code/nemo_experiments/megatron_gpt_peft_lora_tuning/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=0.215-step=30-consumed_samples=30.0.ckpt\n",
      "[NeMo I 2024-03-20 15:34:55 nlp_overrides:463] Removing checkpoint: /mnt/code/nemo_experiments/megatron_gpt_peft_lora_tuning/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=0.281-step=40-consumed_samples=40.0-last.ckpt\n",
      "Epoch 0: :  59%|█████▉    | 60/101 [00:25<00:17, v_num=0, reduced_train_loss=0.0191, global_step=59.00, consumed_samples=60.00, train_step_timing in s=0.133, val_loss=0.157] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▍         | 1/25 [00:00<00:03,  7.12it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|▊         | 2/25 [00:00<00:02,  8.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|█▏        | 3/25 [00:00<00:02,  8.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|█▌        | 4/25 [00:00<00:02,  9.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|██        | 5/25 [00:00<00:02,  9.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|██▍       | 6/25 [00:00<00:02,  9.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|██▊       | 7/25 [00:00<00:01,  9.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|███▏      | 8/25 [00:00<00:01,  9.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|███▌      | 9/25 [00:01<00:01,  8.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|████      | 10/25 [00:01<00:02,  7.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|████▍     | 11/25 [00:01<00:01,  7.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████▊     | 12/25 [00:01<00:01,  7.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|█████▏    | 13/25 [00:01<00:01,  7.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████▌    | 14/25 [00:01<00:01,  7.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|██████    | 15/25 [00:02<00:01,  6.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████▍   | 16/25 [00:02<00:01,  6.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████▊   | 17/25 [00:02<00:01,  6.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|███████▏  | 18/25 [00:02<00:01,  6.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|███████▌  | 19/25 [00:02<00:00,  6.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████  | 20/25 [00:02<00:00,  6.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|████████▍ | 21/25 [00:02<00:00,  7.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|████████▊ | 22/25 [00:03<00:00,  7.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|█████████▏| 23/25 [00:03<00:00,  7.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|█████████▌| 24/25 [00:03<00:00,  6.97it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 25/25 [00:03<00:00,  7.06it/s]\u001b[A\n",
      "Epoch 0: :  59%|█████▉    | 60/101 [00:28<00:19, v_num=0, reduced_train_loss=0.0191, global_step=59.00, consumed_samples=60.00, train_step_timing in s=0.133, val_loss=0.117]\n",
      "                                                  \u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 60: 'validation_loss' reached 0.11653 (best 0.11653), saving model to '/mnt/code/nemo_experiments/megatron_gpt_peft_lora_tuning/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=0.117-step=60-consumed_samples=60.0.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-03-20 15:35:00 nlp_overrides:463] Removing checkpoint: /mnt/code/nemo_experiments/megatron_gpt_peft_lora_tuning/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=0.157-step=50-consumed_samples=50.0.ckpt\n",
      "[NeMo I 2024-03-20 15:35:01 nlp_overrides:463] Removing checkpoint: /mnt/code/nemo_experiments/megatron_gpt_peft_lora_tuning/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=0.157-step=50-consumed_samples=50.0-last.ckpt\n",
      "Epoch 0: :  69%|██████▉   | 70/101 [00:31<00:13, v_num=0, reduced_train_loss=0.0859, global_step=69.00, consumed_samples=70.00, train_step_timing in s=0.129, val_loss=0.117] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▍         | 1/25 [00:00<00:03,  7.32it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|▊         | 2/25 [00:00<00:02,  9.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|█▏        | 3/25 [00:00<00:02, 10.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|█▌        | 4/25 [00:00<00:02,  8.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|██        | 5/25 [00:00<00:02,  9.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|██▍       | 6/25 [00:00<00:02,  9.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|██▊       | 7/25 [00:00<00:01,  9.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|███▏      | 8/25 [00:00<00:01,  9.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|███▌      | 9/25 [00:00<00:01, 10.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|████      | 10/25 [00:00<00:01, 10.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|████▍     | 11/25 [00:01<00:01, 10.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████▊     | 12/25 [00:01<00:01, 10.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|█████▏    | 13/25 [00:01<00:01, 10.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████▌    | 14/25 [00:01<00:01, 10.25it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|██████    | 15/25 [00:01<00:00, 10.24it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████▍   | 16/25 [00:01<00:00, 10.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████▊   | 17/25 [00:01<00:00, 10.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|███████▏  | 18/25 [00:01<00:00, 10.25it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|███████▌  | 19/25 [00:01<00:00, 10.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████  | 20/25 [00:01<00:00, 10.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|████████▍ | 21/25 [00:02<00:00,  8.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|████████▊ | 22/25 [00:02<00:00,  7.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|█████████▏| 23/25 [00:02<00:00,  8.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|█████████▌| 24/25 [00:02<00:00,  8.10it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 25/25 [00:03<00:00,  8.18it/s]\u001b[A\n",
      "Epoch 0: :  69%|██████▉   | 70/101 [00:34<00:15, v_num=0, reduced_train_loss=0.0859, global_step=69.00, consumed_samples=70.00, train_step_timing in s=0.129, val_loss=0.234]\n",
      "                                                  \u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 70: 'validation_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-03-20 15:35:06 nlp_overrides:463] Removing checkpoint: /mnt/code/nemo_experiments/megatron_gpt_peft_lora_tuning/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=0.117-step=60-consumed_samples=60.0-last.ckpt\n",
      "Epoch 0: :  79%|███████▉  | 80/101 [00:36<00:09, v_num=0, reduced_train_loss=0.00297, global_step=79.00, consumed_samples=80.00, train_step_timing in s=0.129, val_loss=0.234]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▍         | 1/25 [00:00<00:09,  2.41it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|▊         | 2/25 [00:00<00:05,  3.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|█▏        | 3/25 [00:00<00:04,  4.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|█▌        | 4/25 [00:00<00:03,  5.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|██        | 5/25 [00:00<00:03,  6.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|██▍       | 6/25 [00:00<00:02,  6.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|██▊       | 7/25 [00:01<00:03,  4.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|███▏      | 8/25 [00:02<00:04,  3.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|███▌      | 9/25 [00:02<00:03,  4.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|████      | 10/25 [00:02<00:03,  4.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|████▍     | 11/25 [00:02<00:02,  4.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████▊     | 12/25 [00:02<00:03,  4.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|█████▏    | 13/25 [00:03<00:02,  4.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████▌    | 14/25 [00:03<00:02,  4.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|██████    | 15/25 [00:03<00:02,  4.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████▍   | 16/25 [00:03<00:01,  4.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████▊   | 17/25 [00:03<00:01,  4.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|███████▏  | 18/25 [00:03<00:01,  5.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|███████▌  | 19/25 [00:03<00:01,  5.25it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████  | 20/25 [00:03<00:00,  5.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|████████▍ | 21/25 [00:03<00:00,  5.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|████████▊ | 22/25 [00:03<00:00,  5.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|█████████▏| 23/25 [00:03<00:00,  5.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|█████████▌| 24/25 [00:04<00:00,  5.91it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 25/25 [00:04<00:00,  5.99it/s]\u001b[A\n",
      "Epoch 0: :  79%|███████▉  | 80/101 [00:40<00:10, v_num=0, reduced_train_loss=0.00297, global_step=79.00, consumed_samples=80.00, train_step_timing in s=0.129, val_loss=0.188]\n",
      "                                                  \u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 80: 'validation_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-03-20 15:35:12 nlp_overrides:463] Removing checkpoint: /mnt/code/nemo_experiments/megatron_gpt_peft_lora_tuning/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=0.234-step=70-consumed_samples=70.0-last.ckpt\n",
      "Epoch 0: :  89%|████████▉ | 90/101 [00:41<00:05, v_num=0, reduced_train_loss=0.258, global_step=89.00, consumed_samples=90.00, train_step_timing in s=0.126, val_loss=0.188]   \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▍         | 1/25 [00:00<00:01, 13.40it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|▊         | 2/25 [00:00<00:01, 13.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|█▏        | 3/25 [00:00<00:01, 12.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|█▌        | 4/25 [00:00<00:01, 12.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|██        | 5/25 [00:00<00:01, 12.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|██▍       | 6/25 [00:00<00:01, 12.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|██▊       | 7/25 [00:00<00:01, 10.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|███▏      | 8/25 [00:00<00:01, 10.25it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|███▌      | 9/25 [00:00<00:01, 10.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|████      | 10/25 [00:00<00:01, 10.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|████▍     | 11/25 [00:01<00:01, 10.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████▊     | 12/25 [00:01<00:01, 10.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|█████▏    | 13/25 [00:01<00:01, 10.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████▌    | 14/25 [00:01<00:01, 10.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|██████    | 15/25 [00:01<00:00, 10.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████▍   | 16/25 [00:01<00:00, 10.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████▊   | 17/25 [00:01<00:00, 10.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|███████▏  | 18/25 [00:01<00:00, 10.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|███████▌  | 19/25 [00:01<00:00, 10.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████  | 20/25 [00:01<00:00, 10.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|████████▍ | 21/25 [00:01<00:00, 10.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|████████▊ | 22/25 [00:02<00:00, 10.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|█████████▏| 23/25 [00:02<00:00, 10.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|█████████▌| 24/25 [00:02<00:00, 10.57it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 25/25 [00:02<00:00, 10.55it/s]\u001b[A\n",
      "Epoch 0: :  89%|████████▉ | 90/101 [00:44<00:05, v_num=0, reduced_train_loss=0.258, global_step=89.00, consumed_samples=90.00, train_step_timing in s=0.126, val_loss=0.200]\n",
      "                                                  \u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 90: 'validation_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-03-20 15:35:16 nlp_overrides:463] Removing checkpoint: /mnt/code/nemo_experiments/megatron_gpt_peft_lora_tuning/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=0.188-step=80-consumed_samples=80.0-last.ckpt\n",
      "Epoch 0: :  99%|█████████▉| 100/101 [00:46<00:00, v_num=0, reduced_train_loss=0.0218, global_step=99.00, consumed_samples=100.0, train_step_timing in s=0.126, val_loss=0.200]  \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▍         | 1/25 [00:00<00:02,  8.49it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|▊         | 2/25 [00:00<00:02,  9.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|█▏        | 3/25 [00:00<00:02,  9.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|█▌        | 4/25 [00:00<00:02, 10.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|██        | 5/25 [00:00<00:02,  9.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|██▍       | 6/25 [00:00<00:01, 10.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|██▊       | 7/25 [00:00<00:01,  9.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|███▏      | 8/25 [00:00<00:01,  9.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|███▌      | 9/25 [00:00<00:01,  9.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|████      | 10/25 [00:01<00:01,  9.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|████▍     | 11/25 [00:01<00:01,  9.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████▊     | 12/25 [00:01<00:01,  9.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|█████▏    | 13/25 [00:01<00:01,  9.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████▌    | 14/25 [00:01<00:01,  9.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|██████    | 15/25 [00:01<00:01,  9.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████▍   | 16/25 [00:01<00:00,  9.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████▊   | 17/25 [00:01<00:00,  9.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|███████▏  | 18/25 [00:01<00:00,  9.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|███████▌  | 19/25 [00:01<00:00,  9.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████  | 20/25 [00:02<00:00,  9.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|████████▍ | 21/25 [00:02<00:00, 10.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|████████▊ | 22/25 [00:02<00:00,  9.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|█████████▏| 23/25 [00:02<00:00,  9.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|█████████▌| 24/25 [00:02<00:00,  9.95it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 25/25 [00:02<00:00,  9.22it/s]\u001b[A\n",
      "Epoch 0: :  99%|█████████▉| 100/101 [00:48<00:00, v_num=0, reduced_train_loss=0.0218, global_step=99.00, consumed_samples=100.0, train_step_timing in s=0.126, val_loss=0.192]\n",
      "                                                  \u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 100: 'validation_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-03-20 15:35:20 nlp_overrides:463] Removing checkpoint: /mnt/code/nemo_experiments/megatron_gpt_peft_lora_tuning/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=0.200-step=90-consumed_samples=90.0-last.ckpt\n",
      "Epoch 0: :  99%|█████████▉| 100/101 [00:49<00:00, v_num=0, reduced_train_loss=0.0218, global_step=99.00, consumed_samples=100.0, train_step_timing in s=0.126, val_loss=0.192]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: :  99%|█████████▉| 100/101 [00:49<00:00, v_num=0, reduced_train_loss=0.0218, global_step=99.00, consumed_samples=100.0, train_step_timing in s=0.126, val_loss=0.192]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /mnt/code/nemo_experiments/megatron_gpt_peft_lora_tuning/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=0.117-step=60-consumed_samples=60.0.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restored all states from the checkpoint at /mnt/code/nemo_experiments/megatron_gpt_peft_lora_tuning/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=0.117-step=60-consumed_samples=60.0.ckpt\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2598875-49ca-4ab8-bdcd-bcca493ab6b7",
   "metadata": {},
   "source": [
    "# Evaluate\n",
    "Now that we have finished fine-tuning, let's try to make some predictions on it from our test dataset.\n",
    "\n",
    "### Load config\n",
    "Just like with fine-tuning, we have prepared a config for this project template. Let's start by loading that in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61a71e7b-9663-48a3-ba6c-d3bac3180be2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config_eval = OmegaConf.load(\"/mnt/code/conf/nemotron-eval-config.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4f68e5-405d-42c2-b07f-b6ba15c0d707",
   "metadata": {},
   "source": [
    "We will override the model path with the last checkpoint that was logged during fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "940b0214-1c59-4462-87f4-c6d2ffe28177",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH=\"/mnt/code/nemo_experiments/megatron_gpt_peft_lora_tuning/checkpoints/megatron_gpt_peft_lora_tuning.nemo\"\n",
    "config_eval.model.restore_from_path=MODEL_PATH\n",
    "config_eval.model.peft.restore_from_path=CHECKPOINT_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42742ec-bbb3-4035-b780-ce9dde94bf4e",
   "metadata": {},
   "source": [
    "### Load model\n",
    "Now we load in the model and trainer that we will use for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4380c787-5943-4345-b7f5-2068321a8924",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-03-20 15:35:44 megatron_trainer_builder:51] Detected interactive environment, using NLPDDPStrategyNotebook\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-03-20 15:35:44 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/lightning_fabric/connector.py:554: UserWarning: 16 is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "      rank_zero_warn(\n",
      "    \n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "[NeMo W 2024-03-20 15:36:07 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:36:07 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:36:07 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:36:07 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:36:07 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:36:07 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:36:07 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:36:07 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:36:07 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:36:07 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:36:07 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:36:07 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:36:07 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:36:07 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:36:07 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:36:07 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:36:07 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:36:07 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:36:07 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-03-20 15:36:07 megatron_init:241] Rank 0 has data parallel group : [0]\n",
      "[NeMo I 2024-03-20 15:36:07 megatron_init:247] Rank 0 has combined group of data parallel and context parallel : [0]\n",
      "[NeMo I 2024-03-20 15:36:07 megatron_init:252] All data parallel group ranks with context parallel combined: [[0]]\n",
      "[NeMo I 2024-03-20 15:36:07 megatron_init:255] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2024-03-20 15:36:07 megatron_init:272] Rank 0 has context parallel group: [0]\n",
      "[NeMo I 2024-03-20 15:36:07 megatron_init:275] All context parallel group ranks: [[0]]\n",
      "[NeMo I 2024-03-20 15:36:07 megatron_init:276] Ranks 0 has context parallel rank: 0\n",
      "[NeMo I 2024-03-20 15:36:07 megatron_init:287] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2024-03-20 15:36:07 megatron_init:288] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-03-20 15:36:07 megatron_init:298] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2024-03-20 15:36:07 megatron_init:302] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-03-20 15:36:07 megatron_init:303] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2024-03-20 15:36:07 megatron_init:317] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2024-03-20 15:36:07 megatron_init:329] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2024-03-20 15:36:07 megatron_init:335] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-03-20 15:36:07 megatron_init:336] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2024-03-20 15:36:07 megatron_init:337] All embedding group ranks: [[0]]\n",
      "[NeMo I 2024-03-20 15:36:07 megatron_init:338] Rank 0 has embedding rank: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[NeMo W 2024-03-20 15:36:07 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:36:07 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:36:07 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:36:07 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:36:07 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:36:07 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:36:07 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:36:07 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:36:07 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:36:07 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:36:07 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:36:07 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:36:07 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:36:07 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:36:07 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:36:07 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:36:07 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:36:07 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:36:07 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-03-20 15:36:07 tokenizer_utils:191] Getting SentencePiece with model: /tmp/tmpqprlt4lc/586f3f51a9cf43bc9369bd53fa08868c_a934dc7c3e1e46a6838bb63379916563_3feba89c944047c19d5a1d0c07a85c32_mt_nlg_plus_multilingual_ja_zh_the_stack_frac_015_256k.model\n",
      "[NeMo I 2024-03-20 15:36:07 megatron_base_model:539] Padded vocab_size: 256000, original vocab_size: 256000, dummy tokens: 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-03-20 15:36:07 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:36:07 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:36:07 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:36:07 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:36:07 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:36:07 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:36:07 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:36:07 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:36:07 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:36:07 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:36:07 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:36:07 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:36:07 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:36:07 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:36:07 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:36:07 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:36:07 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:36:07 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:36:07 megatron_base_model:1104] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:36:07 megatron_base_model:511] The model: MegatronGPTSFTModel() does not have field.name: num_query_groups in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:36:07 megatron_base_model:511] The model: MegatronGPTSFTModel() does not have field.name: add_qkv_bias in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:36:07 megatron_base_model:511] The model: MegatronGPTSFTModel() does not have field.name: num_moe_experts in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:36:07 megatron_base_model:511] The model: MegatronGPTSFTModel() does not have field.name: rotary_interleaved in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:36:07 megatron_base_model:511] The model: MegatronGPTSFTModel() does not have field.name: window_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:36:07 megatron_base_model:511] The model: MegatronGPTSFTModel() does not have field.name: memory_efficient_layer_norm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:36:07 megatron_base_model:511] The model: MegatronGPTSFTModel() does not have field.name: fp8_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:36:07 megatron_base_model:511] The model: MegatronGPTSFTModel() does not have field.name: clone_scatter_output_in_embedding in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:36:07 megatron_base_model:511] The model: MegatronGPTSFTModel() does not have field.name: moe_router_load_balancing_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:36:07 megatron_base_model:511] The model: MegatronGPTSFTModel() does not have field.name: moe_router_topk in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:36:07 megatron_base_model:511] The model: MegatronGPTSFTModel() does not have field.name: moe_grouped_gemm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:36:07 megatron_base_model:511] The model: MegatronGPTSFTModel() does not have field.name: moe_aux_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:36:07 megatron_base_model:511] The model: MegatronGPTSFTModel() does not have field.name: moe_z_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:36:07 megatron_base_model:511] The model: MegatronGPTSFTModel() does not have field.name: moe_input_jitter_eps in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-20 15:36:07 megatron_base_model:511] The model: MegatronGPTSFTModel() does not have field.name: moe_token_dropping in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-03-20 15:36:07 build_model:143]  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 8540135424\n",
      "Loading distributed checkpoint with TensorStoreLoadShardedStrategy\n",
      "[NeMo I 2024-03-20 15:37:43 nlp_overrides:1108] Model MegatronGPTSFTModel was successfully restored from /mnt/artifacts/nemotron/Nemotron-3-8B-Base-4k.nemo.\n",
      "[NeMo I 2024-03-20 15:37:43 nlp_adapter_mixins:184] Before adding PEFT params:\n",
      "      | Name  | Type     | Params\n",
      "    -----------------------------------\n",
      "    0 | model | GPTModel | 8.5 B \n",
      "    -----------------------------------\n",
      "    0         Trainable params\n",
      "    8.5 B     Non-trainable params\n",
      "    8.5 B     Total params\n",
      "    34,160.542Total estimated model params size (MB)\n",
      "[NeMo I 2024-03-20 15:37:45 nlp_adapter_mixins:197] After adding PEFT params:\n",
      "      | Name  | Type     | Params\n",
      "    -----------------------------------\n",
      "    0 | model | GPTModel | 8.6 B \n",
      "    -----------------------------------\n",
      "    16.8 M    Trainable params\n",
      "    8.5 B     Non-trainable params\n",
      "    8.6 B     Total params\n",
      "    34,227.651Total estimated model params size (MB)\n",
      "Parameter count manually:\n",
      "   | Name  | Type     | Params\n",
      "-----------------------------------\n",
      "0 | model | GPTModel | 8.6 B \n",
      "-----------------------------------\n",
      "0         Trainable params\n",
      "8.6 B     Non-trainable params\n",
      "8.6 B     Total params\n",
      "34,227.651Total estimated model params size (MB)\n"
     ]
    }
   ],
   "source": [
    "from nemo.collections.nlp.parts.megatron_trainer_builder import MegatronTrainerBuilder\n",
    "from nemo.collections.nlp.models.language_modeling.megatron_gpt_sft_model import MegatronGPTSFTModel\n",
    "from nemo.collections.nlp.parts.peft_config import LoraPEFTConfig\n",
    "\n",
    "trainer_eval = MegatronTrainerBuilder(config_eval).create_trainer()\n",
    "eval_model_cfg = MegatronGPTSFTModel.merge_inference_cfg(config_eval.model.peft.restore_from_path, config_eval)\n",
    "model_eval = MegatronGPTSFTModel.restore_from(config_eval.model.restore_from_path, eval_model_cfg, trainer=trainer_eval)\n",
    "model_eval.load_adapters(config_eval.model.peft.restore_from_path)\n",
    "model_eval.freeze()\n",
    "\n",
    "print(\"Parameter count manually:\\n\", model_eval.summarize())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cf334c-2131-4765-a5bd-372f1fc74afa",
   "metadata": {},
   "source": [
    "### Load test dataset\n",
    "We load in the test dataset as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14eb9aff-7412-4aef-93f1-3227e05e6593",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-03-20 15:37:45 text_memmap_dataset:116] Building data files\n",
      "[NeMo I 2024-03-20 15:37:45 text_memmap_dataset:525] Processing 1 data files using 128 workers\n",
      "[NeMo I 2024-03-20 15:38:34 text_memmap_dataset:535] Time building 0 / 1 mem-mapped files: 0:00:48.596801\n",
      "[NeMo I 2024-03-20 15:38:34 text_memmap_dataset:525] Processing 1 data files using 128 workers\n",
      "[NeMo I 2024-03-20 15:39:20 text_memmap_dataset:535] Time building 0 / 1 mem-mapped files: 0:00:46.594789\n",
      "[NeMo I 2024-03-20 15:39:20 text_memmap_dataset:158] Loading data files\n",
      "[NeMo I 2024-03-20 15:39:20 text_memmap_dataset:249] Loading /mnt/code/data/SQuAD/squad_short_val.jsonl\n",
      "[NeMo I 2024-03-20 15:39:20 text_memmap_dataset:161] Time loading 1 mem-mapped files: 0:00:00.001306\n",
      "[NeMo I 2024-03-20 15:39:20 text_memmap_dataset:165] Computing global indices\n"
     ]
    }
   ],
   "source": [
    "_test_ds = model_eval._build_dataset(eval_model_cfg.data.test_ds, is_train=False)\n",
    "from torch.utils.data import DataLoader\n",
    "request_dl = DataLoader(\n",
    "    dataset=_test_ds[0],\n",
    "    batch_size=eval_model_cfg.data.test_ds.global_batch_size,\n",
    "    collate_fn=_test_ds[0].collate_fn,\n",
    ")\n",
    "config_inference = OmegaConf.to_container(config_eval.inference, resolve=True)\n",
    "model_eval.set_inference_config(config_inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f13a684-1301-49e6-b084-7bb547125b1c",
   "metadata": {},
   "source": [
    "### Run predictions\n",
    "And now it is time to run the predictions through the model and see the results!\n",
    "\n",
    "**Keep in mind the results you see may vary in quality. The hyperparameters presented in this notebook are not optimal and only serve as examples. Could you be underfitting? Overfitting? These can be adjusted in the configs to improve performance. The point is fine tuning the out-of-the-box model to the general QA task is easy and straightforward with this workflow!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d0b7f22-ba26-49cc-a667-d0f46f16d4c1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[NeMo W 2024-03-20 15:39:20 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:438: PossibleUserWarning: The dataloader, predict_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "      rank_zero_warn(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:   0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-03-20 15:39:21 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/modules/common/text_generation_utils.py:395: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "      string_tensor = torch.as_tensor(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 25/25 [00:10<00:00,  2.32it/s]\n",
      "User: Context:Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50. Question:Which NFL team represented the AFC at Super Bowl 50?\n",
      "\n",
      "Assistant: Denver Broncos\n",
      "\n",
      "\n",
      "User: Context:Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50. Question:Which NFL team represented the NFC at Super Bowl 50?\n",
      "\n",
      "Assistant: Carolina Panthers\n",
      "\n",
      "\n",
      "User: Context:Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50. Question:Where did Super Bowl 50 take place?\n",
      "\n",
      "Assistant: Levi's Stadium\n",
      "\n",
      "\n",
      "User: Context:Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50. Question:Which NFL team won Super Bowl 50?\n",
      "\n",
      "Assistant: Denver Broncos\n",
      "\n",
      "\n",
      "User: Context:Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50. Question:What color was used to emphasize the 50th anniversary of the Super Bowl?\n",
      "\n",
      "Assistant: gold\n",
      "\n",
      "\n",
      "User: Context:Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50. Question:What was the theme of Super Bowl 50?\n",
      "\n",
      "Assistant: golden anniversary\n",
      "\n",
      "\n",
      "User: Context:Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50. Question:What day was the game played on?\n",
      "\n",
      "Assistant: February 7, 2016\n",
      "\n",
      "\n",
      "User: Context:Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50. Question:What is the AFC short for?\n",
      "\n",
      "Assistant: American Football Conference\n",
      "\n",
      "\n",
      "User: Context:Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50. Question:What was the theme of Super Bowl 50?\n",
      "\n",
      "Assistant: golden anniversary\n",
      "\n",
      "\n",
      "User: Context:Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50. Question:What does AFC stand for?\n",
      "\n",
      "Assistant: American Football Conference\n",
      "\n",
      "\n",
      "User: Context:Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50. Question:What day was the Super Bowl played on?\n",
      "\n",
      "Assistant: February 7, 2016\n",
      "\n",
      "\n",
      "User: Context:Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50. Question:Who won Super Bowl 50?\n",
      "\n",
      "Assistant: Denver Broncos\n",
      "\n",
      "\n",
      "User: Context:Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50. Question:What venue did Super Bowl 50 take place in?\n",
      "\n",
      "Assistant: Levi's Stadium\n",
      "\n",
      "\n",
      "User: Context:Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50. Question:What city did Super Bowl 50 take place in?\n",
      "\n",
      "Assistant: Santa Clara\n",
      "\n",
      "\n",
      "User: Context:Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50. Question:If Roman numerals were used, what would Super Bowl 50 have been called?\n",
      "\n",
      "Assistant: Super Bowl L\n",
      "\n",
      "\n",
      "User: Context:Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50. Question:Super Bowl 50 decided the NFL champion for what season?\n",
      "\n",
      "Assistant: 2015\n",
      "\n",
      "\n",
      "User: Context:Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50. Question:What year did the Denver Broncos secure a Super Bowl title for the third time?\n",
      "\n",
      "Assistant: 2016\n",
      "\n",
      "\n",
      "User: Context:Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50. Question:What city did Super Bowl 50 take place in?\n",
      "\n",
      "Assistant: Santa Clara\n",
      "\n",
      "\n",
      "User: Context:Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50. Question:What stadium did Super Bowl 50 take place in?\n",
      "\n",
      "Assistant: Levi's Stadium\n",
      "\n",
      "\n",
      "User: Context:Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50. Question:What was the final score of Super Bowl 50? \n",
      "\n",
      "Assistant: 24–10\n",
      "\n",
      "\n",
      "User: Context:Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50. Question:What month, day and year did Super Bowl 50 take place? \n",
      "\n",
      "Assistant: February 7, 2016\n",
      "\n",
      "\n",
      "User: Context:Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50. Question:What year was Super Bowl 50?\n",
      "\n",
      "Assistant: 2016\n",
      "\n",
      "\n",
      "User: Context:Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50. Question:What team was the AFC champion?\n",
      "\n",
      "Assistant: Denver Broncos\n",
      "\n",
      "\n",
      "User: Context:Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50. Question:What team was the NFC champion?\n",
      "\n",
      "Assistant: Carolina Panthers\n",
      "\n",
      "\n",
      "User: Context:Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50. Question:Who won Super Bowl 50?\n",
      "\n",
      "Assistant: Denver Broncos\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = trainer_eval.predict(model_eval, request_dl)\n",
    "for batch in response:\n",
    "    for s in batch['sentences']:\n",
    "        print(f\"{s}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c651b84a-c343-40d2-9cd9-4a07b241918f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
