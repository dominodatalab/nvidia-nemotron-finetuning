name: megatron_gpt_peft_${model.peft.peft_scheme}_tuning
trainer:
  devices: 1
  accelerator: gpu
  num_nodes: 1
  precision: 32
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  max_epochs: 4
  max_steps: 1000
  log_every_n_steps: 10
  val_check_interval: 50
  gradient_clip_val: 1.0
exp_manager:
  explicit_log_dir: null
  exp_dir: null
  name: ${name}
  create_wandb_logger: false
  wandb_logger_kwargs:
    project: null
    name: null
  resume_if_exists: true
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: true
  checkpoint_callback_params:
    monitor: validation_${model.data.validation_ds.metric.name}
    save_top_k: 1
    mode: min
    save_nemo_on_train_end: true
    filename: ${name}--{${exp_manager.checkpoint_callback_params.monitor}:.3f}-{step}-{consumed_samples}
    model_parallel_size: ${model.tensor_model_parallel_size}
    always_save_nemo: false
    save_best_model: true
  create_early_stopping_callback: false
  early_stopping_callback_params:
    monitor: val_loss
    mode: min
    min_delta: 0.001
    patience: 10
    verbose: true
    strict: false
model:
  seed: 1234
  tensor_model_parallel_size: 1
  pipeline_model_parallel_size: 1
  global_batch_size: 1
  micro_batch_size: 1
  restore_from_path: /domino/edv/dgx-models/Nemotron-3-8B-Base-4k.nemo
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: false
  sequence_parallel: false
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  peft:
    peft_scheme: lora
    restore_from_path: /domino/edv/dgx-models/Nemotron-3-8B-Base-4k.nemo
    adapter_tuning:
      type: parallel_adapter
      adapter_dim: 32
      adapter_dropout: 0.0
      norm_position: pre
      column_init_method: xavier
      row_init_method: zero
      norm_type: mixedfusedlayernorm
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
    lora_tuning:
      adapter_dim: 32
      adapter_dropout: 0.0
      column_init_method: xavier
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
    p_tuning:
      virtual_tokens: 10
      bottleneck_dim: 1024
      embedding_dim: 1024
      init_std: 0.023
    ia3_tuning:
      layer_selection: null
    selective_tuning:
      tunable_base_param_names:
      - self_attention
      - word_embeddings
  data:
    train_ds:
      file_names:
      - /mnt/data/SQuAD/squad_short_train.jsonl
      global_batch_size: ${model.global_batch_size}
      micro_batch_size: ${model.micro_batch_size}
      shuffle: true
      num_workers: 0
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 2048
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      index_mapping_dir: null
      prompt_template: '{input} {output}'
      truncation_method: right
    validation_ds:
      file_names:
      - /mnt/data/SQuAD/squad_short_val.jsonl
      names:
      - squad_val
      global_batch_size: ${model.global_batch_size}
      micro_batch_size: ${model.micro_batch_size}
      shuffle: false
      num_workers: 0
      memmap_workers: ${model.data.train_ds.memmap_workers}
      pin_memory: true
      max_seq_length: 2048
      min_seq_length: 1
      drop_last: false
      label_key: ${model.data.train_ds.label_key}
      add_eos: ${model.data.train_ds.add_eos}
      add_sep: ${model.data.train_ds.add_sep}
      add_bos: ${model.data.train_ds.add_bos}
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: ${model.data.train_ds.truncation_field}
      index_mapping_dir: null
      prompt_template: ${model.data.train_ds.prompt_template}
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
    test_ds:
      file_names: 
      - /mnt/data/SQuAD/squad_short_val.jsonl
      names: 
      - squad
      global_batch_size: 1
      micro_batch_size: 1
      shuffle: false
      num_workers: 0
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 2048
      min_seq_length: 1
      drop_last: false
      context_key: input
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: true
      output_file_path_prefix: /mnt/predictions
      truncation_field: input
      index_mapping_dir: null
      prompt_template: '{input} {output}'
      tokens_to_generate: 30
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null   
  apply_rope_fusion: false
  optim:
    name: fused_adam
    lr: 0.0001
    weight_decay: 0.01
    betas:
    - 0.9
    - 0.98
    sched:
      name: CosineAnnealing
      warmup_steps: 50
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
inference:
  greedy: true
  top_k: 0
  top_p: 0.9
  temperature: 1.0
  all_probs: false
  repetition_penalty: 1.2
  min_tokens_to_generate: 0
  compute_logprob: false
  outfile_path: output.txt
  compute_attention_mask: true
